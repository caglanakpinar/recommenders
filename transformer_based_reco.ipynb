{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceb900a-db97-43c7-b067-a705714a8b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.layers import StringLookup\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd4c34e-40d5-4281-8aee-385ba9a750b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe' # or 'notebook' or 'colab' or 'jupyterlab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d576eeb-2abe-4a72-930d-e86a1bb4fb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp import Params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112ae8ba-f5c9-486a-82fc-6ff62e41b4ed",
   "metadata": {},
   "source": [
    "# Data\n",
    "- data set will be used as sample data which is from kaggle dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32227e4-a9f6-49f3-98ac-a8d1547abbd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Data:\n",
    "    data_path = \"/Volumes/PS2000W/instacart-market-basket-analysis/\"\n",
    "    order_products__prior = pd.read_csv(\n",
    "        data_path + \"order_products__prior.csv\")\n",
    "    order_products__train = pd.read_csv(\n",
    "        data_path + \"order_products__train.csv\")\n",
    "    orders = pd.read_csv(data_path + \"orders.csv\")\n",
    "    products = pd.read_csv(data_path + \"products.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8709f2b-4698-40b8-b0df-e225d90ac703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this update is needed when tensorflow detects numbers as intetegers even it is conterted to strings\n",
    "def convert_to_str(x):\n",
    "    return 'b_' + str(int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c96f49b-ec23-4474-b447-fad5dc2b6d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.products['product_id'] = Data.products['product_id'].apply(convert_to_str)\n",
    "Data.products['aisle_id'] = Data.products['aisle_id'].apply(convert_to_str)\n",
    "Data.products['department_id'] = Data.products['department_id'].apply(convert_to_str)\n",
    "Data.orders['user_id'] = Data.orders['user_id'].apply(convert_to_str)\n",
    "Data.order_products__train['product_id'] = Data.order_products__train['product_id'].apply(convert_to_str)\n",
    "Data.order_products__prior['product_id'] = Data.order_products__prior['product_id'].apply(convert_to_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfdd4b2-36d4-4803-b25d-ba17c7ad2cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.train = (\n",
    "    Data.order_products__train\n",
    "    .merge(\n",
    "        Data.products, \n",
    "        on='product_id', \n",
    "        how='left'\n",
    "    ).merge(\n",
    "        Data.orders\n",
    "        .query(\"eval_set == 'train'\")\n",
    "        [[\n",
    "            'order_id', \n",
    "            'user_id', \n",
    "            'order_dow', \n",
    "            'order_hour_of_day', \n",
    "            'order_number'\n",
    "        ]],\n",
    "        on='order_id',\n",
    "        how='left'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c065fcc5-506a-43d5-8cf2-c7bca21ae357",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.prior = (\n",
    "    Data.order_products__prior\n",
    "    .merge(\n",
    "        Data.products, \n",
    "        on='product_id', \n",
    "        how='left'\n",
    "    ).merge(\n",
    "        Data.orders\n",
    "        .query(\"eval_set == 'prior'\")\n",
    "        [[\n",
    "            'order_id', \n",
    "            'user_id', \n",
    "            'order_dow', \n",
    "            'order_hour_of_day', \n",
    "            'order_number'\n",
    "        ]],\n",
    "        on='order_id',\n",
    "        how='left'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f97dc5-eea4-44cc-aa09-5e6356f36d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.train['ts'] =  (\n",
    "    Data.train\n",
    "    .sort_values([\"user_id\", \"order_number\", \"add_to_cart_order\"])\n",
    "    .groupby(\"user_id\")\n",
    "    .cumcount() + 1\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304c63e2-3da0-4b9f-9823-88c67b1ba263",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.train.head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed76272e-3c85-4223-b9d3-ee9e7d87852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.users = pd.DataFrame(\n",
    "    Data.train.user_id.unique().tolist(),\n",
    "    columns=['user_id']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6740f-13ef-4c4a-9431-e19193963ee5",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4829ed-cb6f-45f4-b62f-9da8935be0b5",
   "metadata": {},
   "source": [
    "### product order cnt & product user cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03602c66-3e7c-4bcb-b891-d94958c30894",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_order_cnt = (\n",
    "    Data.order_products__train\n",
    "    .groupby(\"product_id\")\n",
    "    .order_id\n",
    "    .count()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"order_id\": \"p_order_cnt\"})\n",
    "    .sort_values('p_order_cnt', ascending=False)\n",
    ")\n",
    "\n",
    "_min = product_order_cnt.p_order_cnt.min() \n",
    "_max = product_order_cnt.p_order_cnt.max()\n",
    "\n",
    "product_order_cnt['p_order_cnt_norm'] = (\n",
    "    (product_order_cnt['p_order_cnt'] - _min)\n",
    "    / (_max - _min)\n",
    ")\n",
    "product_order_cnt = product_order_cnt.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e358c480-6bee-44f6-b78a-26c0437e679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.order_products__train.merge(\n",
    "    Data.orders[['order_id', 'user_id']],\n",
    "    on='order_id',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad48b015-89f5-4583-830d-2278efeca1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_user_cnt = (\n",
    "    Data.order_products__train.merge(\n",
    "        Data.orders[['order_id', 'user_id']],\n",
    "        on='order_id',\n",
    "        how='left'\n",
    "    )\n",
    "    .groupby(\"product_id\")\n",
    "    .user_id\n",
    "    .agg(pd.Series.nunique)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"user_id\": \"p_user_cnt\"})\n",
    "    .sort_values('p_user_cnt', ascending=False)\n",
    ")\n",
    "\n",
    "_min = product_user_cnt.p_user_cnt.min() \n",
    "_max = product_user_cnt.p_user_cnt.max()\n",
    "\n",
    "product_user_cnt['p_user_cnt_norm'] = (\n",
    "    (product_user_cnt['p_user_cnt'] - _min)\n",
    "    / (_max - _min)\n",
    ")\n",
    "product_user_cnt = product_user_cnt.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646cac18-1c67-453e-beb0-77b496cad2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.products = (\n",
    "    Data.products\n",
    "    .merge(\n",
    "        product_user_cnt[['product_id', 'p_user_cnt']],\n",
    "        on='product_id',\n",
    "        how='left'\n",
    "    ).merge(\n",
    "        product_order_cnt[['product_id', 'p_order_cnt']],\n",
    "        on='product_id',\n",
    "        how='left'\n",
    "    )\n",
    ")\n",
    "Data.products.head(1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada852f-0960-4458-ba14-d3d8b89648ad",
   "metadata": {},
   "source": [
    "### user order cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690d9f01-9585-4d04-82b8-0ca065aa4798",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_order_cnt = (\n",
    "    Data.orders\n",
    "    .groupby(\"user_id\")\n",
    "    .order_id\n",
    "    .agg(pd.Series.nunique)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"order_id\": \"u_order_cnt\"})\n",
    ")\n",
    "fig = px.histogram(user_order_cnt, x=\"u_order_cnt\", nbins=20)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96222138-2582-4987-9e19-87660946039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.users  = Data.users.merge(\n",
    "    user_order_cnt, \n",
    "    on='user_id', \n",
    "    how='left'\n",
    ")\n",
    "Data.users.head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfc684f-587e-4f5a-9d3d-81781202aa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "### user product cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7afac5-7346-42b1-b0e0-51f999269a46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_product_cnt = (\n",
    "    Data.train\n",
    "    .groupby(\"user_id\")\n",
    "    .product_id\n",
    "    .agg(pd.Series.nunique)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"product_id\": \"u_product_cnt\"})\n",
    ")\n",
    "fig = px.histogram(user_product_cnt, x=\"u_product_cnt\", nbins=20)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5345ebe-9176-48c2-8bd9-cab5c9a9bc54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Data.users  = Data.users.merge(\n",
    "    user_product_cnt, \n",
    "    on='user_id', \n",
    "    how='left'\n",
    ")\n",
    "Data.users.head(1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cd8312-e462-4014-b61b-93e8055f3e49",
   "metadata": {},
   "source": [
    "### rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0292b060-0033-4559-ab2e-b68bbec66af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings = (\n",
    "    Data.order_products__train\n",
    "    [['order_id', 'product_id', 'add_to_cart_order', 'reordered']]\n",
    "    .merge(\n",
    "        product_order_cnt[['product_id', 'p_order_cnt_norm']],\n",
    "        on='product_id',\n",
    "        how='left'\n",
    "    )\n",
    "    .merge(\n",
    "        product_user_cnt[['product_id', 'p_user_cnt_norm']],\n",
    "        on='product_id',\n",
    "        how='left'\n",
    "    )\n",
    ")\n",
    "rankings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a75e14-86b2-4e5b-9473-a77c77b67c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings['relevance_scores'] = (\n",
    "    (.1 * rankings['add_to_cart_order'])\n",
    "    + (.3 * rankings['reordered'])\n",
    "    + (.3 * rankings['p_order_cnt_norm'])\n",
    "    + (.3 * rankings['p_user_cnt_norm'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6330317-155c-4228-8521-9d4f68e46ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(rankings, x=\"relevance_scores\", nbins=20)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9669c9a8-d4f8-425e-b970-efd2865e9fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranking(r):\n",
    "    if r <= .5:\n",
    "        return 1\n",
    "    if .5 < r <= 1.5:\n",
    "        return 2\n",
    "    if 1.5 < r <= 2:\n",
    "        return 3\n",
    "    if 2 < r <= 3.5:\n",
    "        return 4\n",
    "    if 3.5 < r:\n",
    "        return 5\n",
    "\n",
    "rankings['rating'] = rankings.relevance_scores.apply(\n",
    "    get_ranking\n",
    ")\n",
    "rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1976a0-3fbd-4301-8b98-47f6066aae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings.groupby(\"rating\").product_id.agg(pd.Series.nunique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39279006-9e0d-4496-9668-8375d95e136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.train = (\n",
    "    Data.train\n",
    "    .merge(\n",
    "        rankings[['order_id', 'product_id', 'rating']], \n",
    "        on=['order_id', 'product_id'], how='left'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970a560d-3ec1-4fe6-8f76-a9d95b7b6608",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data.train.head(1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82211143-acc3-497d-8572-8e1700d63df1",
   "metadata": {},
   "source": [
    "# pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a72d2f3-8695-4b1c-871e-a12c2f1f80c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookups = [\n",
    "    \"user_id\",\n",
    "    \"product_id\",\n",
    "    \"order_dow\",\n",
    "    \"order_number\",\n",
    "    \"order_hour_of_day\",\n",
    "    \"aisle_id\",\n",
    "    \"department_id\"\n",
    "]\n",
    "\n",
    "\n",
    "sequential_features = [\n",
    "    \"product_id\",\n",
    "    \"rating\", \n",
    "]\n",
    "\n",
    "\n",
    "user_features = [\n",
    "    \"user_order_cnt\",\n",
    "    \"order_hour_of_day\",\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "item_features = [\n",
    "    \"p_order_cnt\",\n",
    "    \"p_user_cnt\",\n",
    "    \"department_id\",\n",
    "    \"aisle_id\",\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "sequence_length = 4 \n",
    "step_size = 2\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"user_id\": \"user_id\",\n",
    "    \"item_id\": \"product_id\",\n",
    "    \"sequence_length\": 4,\n",
    "    \"num_heads\": 3,\n",
    "    \"hidden_layers\": 2,\n",
    "    \"hidden_units\": 256,\n",
    "    \"dropout_rate\": 0.1\n",
    "}\n",
    "\n",
    "\n",
    "lookup_features = list(\n",
    "    set(categorical_features) - set(['sequence_ratings'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31ac15-13ff-4f6e-b769-8149d8946b74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cats in lookups:\n",
    "    Data.train[cats] = Data.train[cats].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2924eecb-c4f4-42b8-8fb9-a9995ea6aadc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = (\n",
    "    Data.train\n",
    "    .sort_values([\"user_id\", \"ts\"])\n",
    "    .groupby(\"user_id\") \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99f2cad-cd5c-46b4-b3e9-41d1294dab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(\n",
    "    {\n",
    "        \"user_id\": list(train_data.groups.keys()),\n",
    "        \"product_ids\": list(train_data['product_id'].apply(list)),\n",
    "        \"ratings\": list(train_data['rating'].apply(list))\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c42c63d-4b18-4aec-8b8c-116684468b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_str(seq):\n",
    "    return \",\".join([str(s) for s in seq])\n",
    "\n",
    "    \n",
    "def create_sequences(values, window_size, step_size):    \n",
    "    sequences = []\n",
    "    start_index = 0\n",
    "    while True:\n",
    "        end_index = start_index + window_size\n",
    "        seq = values[start_index:end_index]\n",
    "        if len(seq) < window_size:\n",
    "            seq = values[-window_size:]\n",
    "            if len(seq) < window_size:\n",
    "                seq = seq + ([seq[-1]] * (window_size - len(seq)))\n",
    "                sequences.append(seq)\n",
    "            break\n",
    "        sequences.append(seq)\n",
    "        start_index += step_size\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff46c383-e215-42c5-bba5-801fe90ee900",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for seq in ['product_ids', 'ratings']:\n",
    "    train_data[\"sequence_\"+seq] = train_data[seq].apply(\n",
    "        lambda row: \n",
    "        create_sequences(\n",
    "            row, \n",
    "            sequence_length,\n",
    "            step_size\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461ea17e-3b94-48fd-8fe9-6cadeb30d548",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3817502b-210f-4767-b1b9-e3e6eff7cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = (\n",
    "    train_data\n",
    "    [['user_id', 'sequence_product_ids', 'sequence_ratings']]\n",
    "    .explode(\n",
    "        ['sequence_product_ids', 'sequence_ratings'], \n",
    "    )\n",
    ")\n",
    "\n",
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2ed7d0-9ef9-4f52-a741-ca955cb15a74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data[[\n",
    "    'sequence_product_ids',\n",
    "    'sequence_ratings',\n",
    "    'target_product_id', \n",
    "    'target'\n",
    "]] = train_data.apply(\n",
    "    lambda row:\n",
    "    pd.Series([\n",
    "        row['sequence_product_ids'][:-1],\n",
    "        row['sequence_ratings'][:-1],\n",
    "        row['sequence_product_ids'][-1], \n",
    "        row['sequence_ratings'][-1]\n",
    "    ]),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea397143-c0e0-4071-ba4e-efdf95be0ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b778d6f-364e-40d0-88ef-b0a6e6b1e531",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = (\n",
    "    train_data.merge(\n",
    "        (\n",
    "            Data.products\n",
    "            .rename(columns={\"product_id\": \"target_product_id\"})\n",
    "        ), \n",
    "        on='target_product_id',\n",
    "        how='left'\n",
    "    ).merge(\n",
    "        Data.users,\n",
    "        on='user_id',\n",
    "        how='left'\n",
    "    )\n",
    ")\n",
    "train_data.head(1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f2ac45-1cba-4dd6-8b90-3534b894af18",
   "metadata": {},
   "source": [
    "# feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16d6390-44af-461a-a5d5-11d6077c45fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    'user_id', \n",
    "    'sequence_product_ids', \n",
    "    'sequence_ratings',\n",
    "    'target_product_id',\n",
    "    'aisle_id',\n",
    "    'department_id'\n",
    "]\n",
    "numeric_features = [\n",
    "    'p_user_cnt', \n",
    "    'p_order_cnt', \n",
    "    'u_order_cnt',\n",
    "    'u_product_cnt'\n",
    "]\n",
    "target = \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90f8738-d4b6-434e-a901-643a9deb70f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[\n",
    "    categorical_features \n",
    "    + numeric_features\n",
    "    + [target]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f21bf0-8ad9-4ec9-9ce7-72597648821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in numeric_features+['target']:\n",
    "    train_data[num] = train_data[num].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c8077-90ca-4f7d-9e5e-2d919865ea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del product_order_cnt, product_user_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890f2d45-5f6f-4b2a-8165-602654054153",
   "metadata": {},
   "outputs": [],
   "source": [
    "del user_order_cnt, user_product_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faf5b65-2bf0-43db-95b0-9126b2d0a67a",
   "metadata": {},
   "source": [
    "# train - validation - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c339d2c-8b07-4964-9e43-7ffab6d17610",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c97f3e-248d-4b5d-99b4-aca9aba9e21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_selection = np.random.rand(len(train_data.index)) <= split_ratio\n",
    "val_dataset = train_data[~random_selection]\n",
    "train_dataset = train_data[random_selection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96084e11-18c8-491c-b8ee-cc0ba71383cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.shape, train_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d2142a-7e2a-4c1d-b327-7f9ad6e968a2",
   "metadata": {},
   "source": [
    "# Lookups & Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419f6567-f592-4c52-8ead-f1fe2c495725",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, params, lookup_features):\n",
    "        self.params = params\n",
    "        self.item_id = params.get('item_id')\n",
    "        self.target_item_id = f\"target_{params.get('item_id')}\"\n",
    "        self.sequence_item_ids = 'sequence_' + params.get('item_id') + 's'\n",
    "        self.sequence_length = params.get('sequence_length')\n",
    "        self.positions = tf.range(start=0, limit=self.sequence_length - 1, delta=1)\n",
    "        self.lookups = {}\n",
    "        self.lookup_features = lookup_features\n",
    "        self.item_lookup_features = [self.target_item_id, self.sequence_item_ids]\n",
    "        self.embedding_encoders = {}\n",
    "        self.embedding_dims = {}\n",
    "        self.item_embedding_processor = None\n",
    "        self.position_embedding_encoder = None\n",
    "\n",
    "    @classmethod\n",
    "    def generate(cls, train_data, params, products, lookup_features):\n",
    "        _cls = Encoder(\n",
    "            params=params,\n",
    "            lookup_features=lookup_features\n",
    "        )\n",
    "        _cls.get_lookups(\n",
    "            train_data,\n",
    "            products\n",
    "        )\n",
    "        return _cls\n",
    "\n",
    "    def update_lookups_and_embeddings(self, vocabulary, lookup):\n",
    "        self.lookups[lookup] = StringLookup(\n",
    "            vocabulary=vocabulary, mask_token=None, oov_token=0,  num_oov_indices=1)\n",
    "        self.embedding_dims[lookup] = int(math.sqrt(len(vocabulary)))\n",
    "        self.embedding_encoders[lookup] = layers.Embedding(\n",
    "                input_dim=len(vocabulary)+1,\n",
    "                output_dim=self.embedding_dims[lookup],\n",
    "                name=f\"{lookup}_embedding\",\n",
    "            )\n",
    "\n",
    "    def get_lookups(self, train_data, products):\n",
    "        for lookup in self.lookup_features:\n",
    "            if lookup not in self.item_lookup_features: \n",
    "                # Convert the string input values into integer indices.\n",
    "                vocabulary = train_data[lookup].astype(str).unique().tolist()\n",
    "                self.update_lookups_and_embeddings(vocabulary, lookup)\n",
    "                                              \n",
    "        # item Id embedding and lookups\n",
    "        vocabulary = products[self.item_id].astype(str).unique().tolist()\n",
    "        self.update_lookups_and_embeddings(vocabulary, self.item_id)\n",
    "        self.item_embedding_processor = layers.Dense(\n",
    "            units=self.embedding_dims[self.item_id],\n",
    "            activation=\"relu\",\n",
    "            name=f\"process_{self.item_id}_embedding\",\n",
    "        )\n",
    "        self.position_embedding_encoder = layers.Embedding(\n",
    "            input_dim=self.sequence_length - 1,\n",
    "            output_dim=self.embedding_dims[self.item_id],\n",
    "            name=\"position_embedding\",\n",
    "        )\n",
    "\n",
    "    def query(self, inp, lookup):\n",
    "        return self.embedding_encoders[lookup](inp)\n",
    "\n",
    "    def item_embeddings(self, inputs):\n",
    "        emb_target = self.query(inputs[self.target_item_id], self.item_id)\n",
    "        emb_target = self.item_embedding_processor(emb_target)\n",
    "        emb_seq = self.query(inputs[self.sequence_item_ids], self.item_id)\n",
    "        emb_seq = self.item_embedding_processor(emb_seq)\n",
    "        return emb_target, emb_seq\n",
    "\n",
    "    def get_embeddings(self, inputs):\n",
    "        encoded = []\n",
    "        encoded_transformer = []\n",
    "        for lookup in self.lookup_features:\n",
    "            if lookup not in self.item_lookup_features: \n",
    "                print(inputs[lookup])\n",
    "                encoded.append(self.query(inputs[lookup], lookup))\n",
    "        \n",
    "        ## Create a single embedding vector for the user features\n",
    "        if len(encoded) > 1:\n",
    "            encoded = layers.concatenate(encoded)\n",
    "        elif len(encoded) == 1:\n",
    "            encoded = encoded[0]\n",
    "        else:\n",
    "            encoded = None\n",
    "\n",
    "        (\n",
    "            encoded_target_item, \n",
    "            encoded_sequence_items\n",
    "        ) = self.item_embeddings(\n",
    "            inputs\n",
    "        )        \n",
    "        encodded_positions = self.position_embedding_encoder(self.positions)\n",
    "        sequence_ratings = keras.ops.expand_dims(inputs[\"sequence_ratings\"], -1)\n",
    "\n",
    "        encoded_sequence_items_with_poistion_and_rating = layers.Multiply()(\n",
    "            [(encoded_sequence_items + encodded_positions), sequence_ratings]\n",
    "        )\n",
    "\n",
    "        # Construct the transformer inputs.\n",
    "        for i in range(self.sequence_length - 1):\n",
    "            feature = encoded_sequence_items_with_poistion_and_rating[:, i, ...]\n",
    "            feature = keras.ops.expand_dims(feature, 1)\n",
    "            encoded_transformer.append(feature)\n",
    "\n",
    "        encoded_transformer = layers.concatenate(\n",
    "            encoded_transformer, axis=1\n",
    "        )\n",
    "            \n",
    "        return encoded_transformer, encoded            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc55a04-05cb-4d00-a150-f45f732d2cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = Encoder.generate(\n",
    "    train_data=train_data,\n",
    "    params=params,\n",
    "    products=Data.products,\n",
    "    lookup_features=lookup_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69287b6-bf47-40a9-b61c-673e2949d143",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders.lookups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ea0666-65b3-4328-b2aa-f42a5a65f1cf",
   "metadata": {},
   "source": [
    "# create `tf.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab1c91-1832-4e82-b6c2-89d8a5dc4baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_dataset = {\n",
    "    encoders.target_item_id: tf.reshape(\n",
    "        encoders.lookups[encoders.item_id](train_dataset[encoders.target_item_id]), (len(train_dataset),1)\n",
    "    ),\n",
    "    encoders.sequence_item_ids: (\n",
    "        encoders.lookups[encoders.item_id](train_dataset[encoders.sequence_item_ids].tolist())),\n",
    "    \"sequence_ratings\": tf.cast(train_dataset['sequence_ratings'].tolist(), tf.float32),\n",
    "    \"target\": tf.reshape(tf.cast(train_dataset['target'], tf.float32), (len(train_dataset),1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e91326-02cf-41d3-8a14-b39f04ef1708",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in lookup_features:\n",
    "    if f not in train_feature_dataset:\n",
    "        train_feature_dataset[f] = tf.reshape(\n",
    "            encoders.lookups[f](train_dataset[f]), (len(train_dataset),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8ef61a-6a01-4399-a606-2db3d05e14b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in numeric_features:\n",
    "    train_feature_dataset[f] = tf.reshape(tf.cast(train_dataset[f], tf.float32), (len(train_dataset), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb65df-0b3b-4967-8804-e00ea7c98063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(feature):\n",
    "    target = feature['target']\n",
    "    del feature['target']\n",
    "    return feature, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc90ea2d-d8ce-444f-b01b-c9f6b862ef20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_feature_dataset = tf.data.Dataset.from_tensor_slices(train_feature_dataset)\n",
    "train_feature_dataset = train_feature_dataset.cache()\n",
    "train_feature_dataset = train_feature_dataset.shuffle(5000)\n",
    "train_feature_dataset = train_feature_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "train_feature_dataset = train_feature_dataset.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25851ca5-5e35-4558-ac73-f9df7f5a4a11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_feature_dataset = train_feature_dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ee62ea-a9f7-445d-b288-0c9d0d0d3854",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in train_feature_dataset.take(1):\n",
    "    print()\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee343afe-cb18-4890-9a19-68ef6bd4968c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6629f36c-46a6-41dc-9851-fc87cd885ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inputs:\n",
    "    def __init__(\n",
    "        self, \n",
    "        params,\n",
    "        categorical_features, \n",
    "        numeric_features\n",
    "    ):\n",
    "        self.params = params\n",
    "        self.item_id = params.get('item_id')\n",
    "        self.target_item_id = f\"target_{params.get('item_id')}\"\n",
    "        self.sequence_item_ids = 'sequence_' + params.get('item_id') + 's'\n",
    "        self.sequence_length = params.get('sequence_length')\n",
    "        self.categorical_features = categorical_features\n",
    "        self.numeric_features = numeric_features\n",
    "        self.inputs = {}\n",
    "        self.collect_inputs()\n",
    "\n",
    "    def collect_inputs(self):\n",
    "        for cat in self.categorical_features:\n",
    "            if cat == self.sequence_item_ids:\n",
    "                self.inputs[cat] = keras.Input(\n",
    "                    name=cat, shape=(self.sequence_length - 1,)\n",
    "                )\n",
    "            elif cat == \"sequence_ratings\":\n",
    "                self.inputs[cat] = keras.Input(\n",
    "                    name=\"sequence_ratings\", shape=(self.sequence_length - 1,)\n",
    "                )\n",
    "            else:\n",
    "                self.inputs[cat] = keras.Input(name=cat, shape=(1,))\n",
    "\n",
    "        for num in self.numeric_features:\n",
    "            self.inputs[num] = keras.Input(name=num, shape=(1,)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c175d2-2a67-438e-8360-88a8ba70d077",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs = Inputs(\n",
    "        params,\n",
    "        categorical_features, \n",
    "        numeric_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9cfa9a-686d-4ae8-b882-41861ec3bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44de0ee9-ded2-40bb-80e8-e14e0fdf42a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c770bb2-4d41-4898-9e69-f96e3552cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__(self, params, inputs: Inputs, encoders: Encoder):\n",
    "        self.params = params\n",
    "        self.num_heads = params.get('num_heads')\n",
    "        self.dropout_rate = params.get('dropout_rate')\n",
    "        self.inputs = inputs\n",
    "        self.encoders = encoders\n",
    "        self.hidden_units = self.cal_hidden_layer_of_units(\n",
    "            params.get('hidden_layers'),\n",
    "            params.get('hidden_units')\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def cal_hidden_layer_of_units(\n",
    "        hidden_layers, _encoding_dim, autoencoder_layers=False\n",
    "    ):\n",
    "        \"\"\"creating hidden layers for each tower\n",
    "        hidden_layers:\n",
    "            number of hidden layer that will be created\n",
    "        _encoding_dim:\n",
    "            number of hidden unit that will be used in first hidden layer\n",
    "        autoencoder_layers:\n",
    "            if it is for autoencoder, process will not be same. hidden unit will be decreasing for each hidden layer,\n",
    "            however, for autoencoder, after seeing bottle_neck unit, unit size will be re-increasing till _encoding_dim\n",
    "        how it works;\n",
    "            1st example_configurations;\n",
    "                hidden_layers      : 3\n",
    "                _encoding_dim      : 16\n",
    "                autoencoder_layers : False\n",
    "                layers             : 16 - 8 (16/2) - 4 (8/2) - 2 (4/2)\n",
    "            2nd example_configurations;\n",
    "                hidden_layers      : 3\n",
    "                _encoding_dim      : 16\n",
    "                autoencoder_layers : True\n",
    "                layers             : 16 - 8 (16/2) - 4 (8/2) - 2 (4/2) (bottle_neck) - 4 (2*2) - 8 (4*2) - 16 (8*2)\n",
    "        \"\"\"\n",
    "        count = 1\n",
    "        _unit = _encoding_dim\n",
    "        h_l_units = []\n",
    "        while count != hidden_layers + 1:\n",
    "            h_l_units.append(int(_unit))\n",
    "            _unit /= 2\n",
    "            if int(_unit) == 1:\n",
    "                count = hidden_layers + 1\n",
    "            else:\n",
    "                count += 1\n",
    "        if autoencoder_layers:\n",
    "            count = 1\n",
    "            while count != hidden_layers + 2:\n",
    "                h_l_units.append(int(_unit))\n",
    "                _unit *= 2\n",
    "                count += 1\n",
    "        return h_l_units\n",
    "\n",
    "    def create_model(self):\n",
    "        transformer_features, other_features = self.encoders.get_embeddings(\n",
    "            self.inputs.inputs\n",
    "        )\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=self.num_heads, \n",
    "            key_dim=transformer_features.shape[2], \n",
    "            dropout=self.dropout_rate\n",
    "        )(transformer_features, transformer_features)\n",
    "    \n",
    "        # Transformer block.\n",
    "        attention_output = layers.Dropout(self.dropout_rate)(attention_output)\n",
    "        x1 = layers.Add()([transformer_features, attention_output])\n",
    "        x1 = layers.LayerNormalization()(x1)\n",
    "        x2 = layers.LeakyReLU()(x1)\n",
    "        x2 = layers.Dense(units=x2.shape[-1])(x2)\n",
    "        x2 = layers.Dropout(self.dropout_rate)(x2)\n",
    "        transformer_features = layers.Add()([x1, x2])\n",
    "        transformer_features = layers.LayerNormalization()(transformer_features)\n",
    "        features = layers.Flatten()(transformer_features)\n",
    "    \n",
    "        # Included the other features.\n",
    "        if other_features is not None:\n",
    "            features = layers.concatenate(\n",
    "                [features, layers.Reshape([other_features.shape[-1]])(other_features)]\n",
    "            )\n",
    "    \n",
    "        # Fully-connected layers.\n",
    "        for num_units in self.hidden_units:\n",
    "            features = layers.Dense(num_units)(features)\n",
    "            features = layers.BatchNormalization()(features)\n",
    "            features = layers.LeakyReLU()(features)\n",
    "            features = layers.Dropout(self.dropout_rate)(features)\n",
    "    \n",
    "        outputs = layers.Dense(units=1)(features)\n",
    "        model = keras.Model(inputs=self.inputs.inputs, outputs=outputs)\n",
    "        return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86711543-fac1-4b30-acaf-e5ae7bd2087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    params,\n",
    "    inputs,\n",
    "    encoders\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008fd556-4d29-4d2b-b0d7-bdc1a55e2e4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformer.encoders.get_embeddings(\n",
    "    i[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f2b1f2-a985-4064-82e1-c650fcb9c49b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = transformer.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd97fa03-baa2-4819-949c-ab2d25c49fcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc37c53-830b-47f6-a366-39f85ade2ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adagrad(learning_rate=0.01),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    metrics=[keras.metrics.MeanAbsoluteError()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14058229-dee2-4bc8-9a94-e3838a2a9827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c05e4eb-fee9-421a-9596-d2570a4a4d21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_feature_dataset,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c2e657-d1e7-4cd7-98b9-97d527d70f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d137b5b1-92fd-4fad-921b-7ebd79422536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd1634a-e430-4bd4-94d1-eeebc5ca2559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad8e176-a848-43d4-a978-93f0475d8cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315e4179-cf26-4fcc-984c-e5728e02e634",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
